{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ea5c11-157c-4aee-b6d0-385831f992c8",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #AC4d61; padding: 10px; text-align: center; border-radius: 15px; margin-bottom: 10px;\">\n",
    "    <h1 style=\"color: #ffffff; font-size: 2.3em; font-weight: bold;\">Exploratory Data Analysis and Visualization of Celestial Bodies</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b52b83b-b8f5-440f-ac5b-7d5f38cbc330",
   "metadata": {},
   "source": [
    "<div style=\" border: 5px solid #AC4d61; border-radius: 20px;\">\n",
    "    <h2 style=\"margin: auto; padding: 20px;color: #AC4d61;font-size:1.8em;\">Table of Contents</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f89e2c-57bf-4631-8e31-e191a1cbb0df",
   "metadata": {},
   "source": [
    "**********************************\n",
    "**********************************\n",
    "**1. Introduction**\n",
    "\n",
    "**2. Objectives**\n",
    "\n",
    "**3. Dataset Information**\n",
    "\n",
    "**4. Data Collection**\n",
    "- *4.1 Libraries Used* \n",
    "- *4.2 Data Acquisition*\n",
    "- *4.3 Filtering Relevant URLs*\n",
    "- *4.4 Data Validation*\n",
    "\n",
    "**5. Data Structuring**\n",
    "- *5.1 Text Data* \n",
    "- *5.2 Image Data*\n",
    "\n",
    "**6. Data Loading and Cleaning**\n",
    "- *6.1 Load the Data*\n",
    "- *6.2 Merge DataFrames*\n",
    "- *6.3 Data Cleaning*\n",
    "- *6.4 Save the Merged Data*\n",
    "\n",
    "**7. Exploratory Data Analysis (EDA)**\n",
    "- *7.1 Overview of Textual Data*\n",
    "- *7.2 Overview of Image Data*\n",
    "- *7.3 Trends and Patterns*\n",
    "- *7.4 Correlation between features*\n",
    "  \n",
    "**8. Model Building and Deployment**\n",
    "- *8.1 Data Preparation*\n",
    "- *8.2 Model Selection*\n",
    "- *8.3 Saving the Model*\n",
    "- *8.4 Deployment*\n",
    "\n",
    "**9. Conclusion**\n",
    "**********************************\n",
    "**********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9069d914-bcc4-45fd-a30e-b0f784104672",
   "metadata": {},
   "source": [
    "<div style=\" border: 5px solid #AC4d61; border-radius: 20px;\">\n",
    "    <h2 style=\"margin: auto; padding: 20px;color: #AC4d61;font-size:1.8em;\">1. Introduction</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea60a84f-f402-439d-8601-b369631d6801",
   "metadata": {},
   "source": [
    "***********\n",
    "<p>\n",
    "    The focus of this project is on celestial bodies such as <strong>planets</strong>, <strong>stars</strong>, and <strong>galaxies</strong>. \n",
    "    The objective is to apply advanced <strong>data analysis</strong> and <strong>data science</strong> techniques to extract meaningful insights. \n",
    "    This notebook walks through the process of <strong>collecting</strong>, <strong>structuring</strong>, <strong>exploring</strong>, and <strong>visualizing</strong> data about celestial objects.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    Celestial bodies, ranging from planets to stars and galaxies, provide profound insights into <strong>astrophysical phenomena</strong>. \n",
    "    They play a critical role in humanityâ€™s ongoing quest for knowledge and understanding of the universe. \n",
    "    This project utilizes publicly available resources, such as <strong>Wikipedia</strong>, to explore celestial bodies through <strong>analytical methods</strong> and uncover significant patterns.\n",
    "</p>\n",
    "\n",
    "**************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf178834-e2a4-4250-9714-2c1f3b8c8f54",
   "metadata": {},
   "source": [
    "<div style=\" border: 5px solid #AC4d61; border-radius: 20px;\">\n",
    "    <h2 style=\"margin: auto; padding: 20px;color: #AC4d61;font-size:1.8em;\">2. Objectives</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27ff72a-289c-4cf0-98cd-0fc11a583c45",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><strong>Data Collection</strong> from Wikipedia.</li>\n",
    "    <li><strong>Data Structuring</strong> (text and images) for analysis.</li>\n",
    "    <li><strong>Data Loading and Cleaning</strong> </li>\n",
    "    <li>\n",
    "        Perform <strong>Exploratory Data Analysis (EDA)</strong> \n",
    "    </li>\n",
    "    <li><strong>Model Building and Deployment</strong> </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f413c966-7e4b-4475-8010-08da0bed47f6",
   "metadata": {},
   "source": [
    "<div style=\" border: 5px solid #AC4d61; border-radius: 20px;\">\n",
    "    <h2 style=\"margin: auto; padding: 20px;color: #AC4d61;font-size:1.8em;\">3. Dataset Information</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b987a9b5-ae93-4297-9fa6-64bd93d5376d",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><strong>Dataset Name</strong>: Celestial_Bodies_Dataset</li>\n",
    "    <li><strong>Source</strong>: Publicly available Wikipedia data.</li>\n",
    "    <li><strong>Analysts</strong>: Benseddik Abir and Benzahi Wissal</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Components:</h3>\n",
    "<ol>\n",
    "    <li><strong>URLs</strong>: Web links to celestial body pages.</li>\n",
    "    <li><strong>Text Data</strong>: Descriptions and key features.</li>\n",
    "    <li><strong>Image Data</strong>: Visual representations of celestial objects.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd49ce9-2d29-4057-b5bb-9187eedb9e80",
   "metadata": {},
   "source": [
    "<div style=\" border: 5px solid #AC4d61; border-radius: 20px;\">\n",
    "    <h2 style=\"margin: auto; padding: 20px;color: #AC4d61;font-size:1.8em;\">4. Data Collection</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aab684-26ee-49ca-bca1-436baa0b6d34",
   "metadata": {},
   "source": [
    "## <span style=\"color: #A05899; font-size:1em;\">4.1 Libraries Used</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645564e-5672-417d-9210-5cf4880a19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "import re\n",
    "import urllib.parse\n",
    "from urllib.parse import urljoin\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "\n",
    "#-----------------------------------------------------\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "#-----------------------------------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "#----------------------------------------------------\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e13d6-bbc5-41ab-a330-6aae67f438ed",
   "metadata": {},
   "source": [
    "## <span style=\"color: #A05899; font-size:1em;\">4.2. Data Acquisition</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b6673-c135-43a5-b022-899aad9d5ac0",
   "metadata": {},
   "source": [
    "**Methods:**\n",
    "- Used requests and BeautifulSoup libraries to scrape Wikipedia pages.\n",
    "- **Scope:** Fetch URLs related to celestial objects for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee4041-0326-4abe-a449-216bb964586d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize an empty set to store unique URLs\n",
    "urls_data = set()\n",
    "\n",
    "# Function to fetch related Wikipedia pages for a given topic\n",
    "def fetch_related_pages(topic, limit=1000):\n",
    "    # Wikipedia API endpoint for querying search results\n",
    "    endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
    "    # Use a session for persistent HTTP connections\n",
    "    session = requests.Session()\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": topic,\n",
    "        \"srlimit\": 100,  \n",
    "        \"format\": \"json\",\n",
    "        \"continue\": \"\",\n",
    "    }\n",
    "\n",
    "    # Continue fetching data until the limit is reached\n",
    "    while len(urls_data) < limit:\n",
    "        try:\n",
    "            # Send a GET request to the API with specified parameters\n",
    "            response = session.get(endpoint, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Handle connection errors and stop further requests\n",
    "            print(f\"Request failed: {e}\")\n",
    "            break\n",
    "\n",
    "        # Stop if no results are returned\n",
    "        if 'query' not in data:\n",
    "            print(\"No more results or API limit reached.\")\n",
    "            break\n",
    "\n",
    "        # Record the number of URLs collected before the current request\n",
    "        previous_count = len(urls_data)\n",
    "        \n",
    "        # Extract URLs from the search results and add them to the set\n",
    "        for page in data['query']['search']:\n",
    "            url = f\"https://en.wikipedia.org/wiki/{page['title'].replace(' ', '_')}\"\n",
    "            urls_data.add(url)\n",
    "            print(f\"Collected: {url}\")\n",
    "            # Stop fetching if the limit is reached\n",
    "            if len(urls_data) >= limit:\n",
    "                break\n",
    "\n",
    "        # Stop if no new unique results were found in the current iteration\n",
    "        if len(urls_data) == previous_count:\n",
    "            print(\"No additional unique results found.\")\n",
    "            break\n",
    "\n",
    "        # If there are more results to fetch, update the continuation token\n",
    "        if \"continue\" in data:\n",
    "            params.update(data[\"continue\"])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        # Introduce a delay between requests to avoid rate-limiting by the API\n",
    "        time.sleep(1) \n",
    "# Topic to search for in the Wikipedia API\n",
    "topic = \"Celestial Bodies\"\n",
    "\n",
    "# Call the function to fetch related pages for the specified topic\n",
    "fetch_related_pages(topic, limit=1000)\n",
    "# Save the collected URLs into a DataFrame and export to a CSV file\n",
    "df = pd.DataFrame(list(urls_data), columns=['URL'])\n",
    "df.to_csv('celestial_body.csv', index=False, encoding='utf-8')\n",
    "print(f'Total URLs collected: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681e986-6581-4801-835c-fddf9327d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shape of the DataFrame to verify the number of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf4c41b-f49b-4702-b8a6-6306af826cf1",
   "metadata": {},
   "source": [
    "## <span style=\"color: #A05899; font-size:1em;\">4.3 Filtering Relevant URLs</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f179a3-63d9-4fdd-af7c-ebe548c98e1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Load the CSV file containing URLs\n",
    "df = pd.read_csv('celestial_body.csv')\n",
    "\n",
    "# Define relevant keywords for celestial bodies\n",
    "keywords = [\n",
    "    \"Planet\", \"Star\", \"Moon\", \"Asteroid\", \"Comet\", \"Galaxy\", \"Nebula\", \"Black hole\",\n",
    "    \"Exoplanet\", \"Solar system\", \"Dwarf planet\", \"Satellite\", \"Meteor\", \"Cosmos\",\n",
    "    \"Orbit\", \"Universe\", \"Supernova\", \"Astronomy\", \"Astrophysics\", \"Space\",\n",
    "    \"Milky Way\", \"Pulsar\", \"Quasar\", \"Kuiper Belt\", \"Oort Cloud\", \"White dwarf\",\n",
    "    \"Red giant\", \"Event horizon\", \"Dark matter\", \"Dark energy\", \"Constellation\",\n",
    "    \"Telescope\", \"Astronomical object\", \"Interstellar\", \"Celestial sphere\",\n",
    "    \"Eclipse\", \"Cosmology\", \"Gravitational wave\", \"Big Bang\", \"Space exploration\"\n",
    "]\n",
    "\n",
    "# Function to check if the title contains any relevant keywords\n",
    "def is_celestial_related(url):\n",
    "    try:\n",
    "        # Fetch the page content using the Wikipedia API\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        page_content = response.text.lower()\n",
    "        \n",
    "        # Check if any of the keywords appear in the page title or content\n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in page_content:\n",
    "                return True\n",
    "        return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for {url}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Filter URLs that are related to celestial bodies\n",
    "filtered_urls = []\n",
    "\n",
    "for url in df['URL']:\n",
    "    if is_celestial_related(url):\n",
    "        filtered_urls.append(url)\n",
    "        print(f\"Related URL: {url}\")\n",
    "\n",
    "# Create a new DataFrame with the filtered URLs\n",
    "filtered_df = pd.DataFrame(filtered_urls, columns=['URL'])\n",
    "\n",
    "# Save the filtered URLs to a new CSV file\n",
    "filtered_df.to_csv('filtered_celestial_bodies.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Total filtered URLs collected: {len(filtered_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee568f3-4c6f-4d4a-a124-90619327d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f41d89-75ab-4e39-9e81-936c2e07fbab",
   "metadata": {},
   "source": [
    "## <span style=\"color: #A05899; font-size:1em;\">4.4 Data Validation</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43087f-3485-4485-b1bc-e85900e5d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the filtered dataset\n",
    "filter_df = pd.read_csv('filtered_celestial_bodies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ec7d49-85ce-416c-a68b-5de32b5dce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915aca8d-9168-450d-831b-90bbccdcbcb1",
   "metadata": {},
   "source": [
    "1. **Check for Duplicate Entries**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c06ba2-6ad2-4dfa-b30f-282de493a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate entries\n",
    "duplicate_count = filter_df.duplicated(subset=['URL']).sum()\n",
    "print(f\"Number of duplicate entries: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f61df7-9139-4ee8-897e-b5a320e5ad39",
   "metadata": {},
   "source": [
    "2. **Verify URLs**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca1494-9849-46f8-87ec-8f05c7d52792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any invalid or empty URLs\n",
    "invalid_urls = filter_df[filter_df['URL'].isnull()]\n",
    "print(f\"Number of invalid URLs: {invalid_urls.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aca05b-cfeb-41e3-b08e-6cd37142e2ed",
   "metadata": {},
   "source": [
    "<div style=\" border: 5px solid #AC4d61; border-radius: 20px;\">\n",
    "    <h2 style=\"margin: auto; padding: 20px;color: #AC4d61;font-size:1.8em;\">5. Data Structuring</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f51a05-4204-4aff-b0b3-e3ccbc48e30d",
   "metadata": {},
   "source": [
    "## <span style=\"color: #A05899; font-size:1em;\">5.1  Text Data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4b5c16-b25b-49b8-a949-4ad6e269a592",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">5.1.1 Extracting Titles from URLs:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ec07e-1fd7-434a-863b-2c20df2e1453",
   "metadata": {},
   "source": [
    "**Steps**:\n",
    "1. Extract the last part of each URL as the page title.\n",
    "2. Replace underscores (`_`) with spaces for readability.\n",
    "3. Create a new DataFrame with columns: **`Title`** and **`URL`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6a259a-2ccc-452a-9101-6340a73505d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example filtered_df (Assuming it's already loaded)\n",
    "# filtered_df = pd.DataFrame({'URL': ['https://en.wikipedia.org/wiki/Salvator_Mundi_(painting)', 'https://en.wikipedia.org/wiki/Trabant', ...]})\n",
    "\n",
    "# Create a new column 'Title' by extracting the title from the URL\n",
    "filter_df['Title'] = filter_df['URL'].apply(lambda x: x.split('/')[-1].replace('_', ' '))\n",
    "\n",
    "# Create a new DataFrame with only 'Title' and 'URL' columns\n",
    "df_separat = filter_df[['Title', 'URL']]\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(df_separat.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b4d807-8e96-4bee-8a78-06a94d4d0719",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_separat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24929560-4c87-4dc0-b97d-2300a8e40e5e",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">5.1.2 Extracting and Analyzing Conten:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fad3d28-a302-4094-b643-6865b998610f",
   "metadata": {},
   "source": [
    "---\n",
    " \n",
    "This step involves scraping textual content from URLs, extracting key features like word count, and preparing the data for analysis. The scraped text is also cleaned to ensure uniformity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627fc43b-1ab6-416b-bad8-807331d1929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape content and compute features\n",
    "def scrape_and_analyze(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract content (all paragraphs)\n",
    "        content = ' '.join([p.text.strip() for p in soup.find_all('p') if p.text.strip()])\n",
    "\n",
    "        # Compute features (example: word count)\n",
    "        word_count = len(content.split())\n",
    "        features = f\"Word count: {word_count}\"\n",
    "\n",
    "        return content, features\n",
    "    except Exception as e:\n",
    "        # Handle errors (e.g., timeout, invalid URL)\n",
    "        return \"Error fetching content\", f\"Error: {str(e)}\"\n",
    "\n",
    "# Apply the function to each URL in the DataFrame\n",
    "df_separat ['Content'], df_separat ['Features'] = zip(*df_separat ['URL'].apply(scrape_and_analyze))\n",
    "\n",
    "# Save to CSV (optional)\n",
    "df_separat .to_csv('extended_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc7aeec-783f-4b6e-b0f7-e287a680a2af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_separat = pd.read_csv('extended_data.csv')\n",
    "df_separat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b269e74-0d9f-4673-ba53-61016a900027",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Information:\")\n",
    "df_separat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf082fc-895f-4369-aaa1-05abdafefe9f",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">5.1.3 Data Cleaning and Classification:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf2f7a-f184-4bb0-93f5-3d18a1eae76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Handle missing values in the 'Content' column\n",
    "df_separat['Content'].fillna('', inplace=True)\n",
    "\n",
    "# Clean the 'Content' column\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Ensure the text is a string\n",
    "        return ''\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "    return text\n",
    "\n",
    "df_separat['Cleaned_Content'] = df_separat['Content'].apply(clean_text)\n",
    "\n",
    "# Define celestial body types and keywords\n",
    "celestial_types = {\n",
    "    'Planet': ['planet', 'jupiter', 'earth', 'mars', 'venus', 'saturn', 'uranus', 'neptune', 'dwarf planet'],\n",
    "    'Star': ['star', 'sun', 'nova', 'supernova', 'neutron star', 'red giant', 'pulsar', 'white dwarf'],\n",
    "    'Moon': ['moon', 'satellite', 'luna', 'natural satellite'],\n",
    "    'Asteroid': ['asteroid', 'comet', 'meteoroid', 'meteor', 'meteorite'],\n",
    "    'Galaxy': ['galaxy', 'milky way', 'andromeda', 'spiral galaxy', 'elliptical galaxy', 'irregular galaxy'],\n",
    "    'Nebula': ['nebula', 'emission nebula', 'reflection nebula', 'planetary nebula', 'dark nebula'],\n",
    "    'Black Hole': ['black hole', 'event horizon', 'singularity'],\n",
    "    'Constellation': ['constellation', 'zodiac'],\n",
    "    'Exoplanet': ['exoplanet', 'extrasolar planet'],\n",
    "    'Cosmic Structure': ['dark matter', 'dark energy', 'cosmos', 'universe', 'kuiper belt', 'oort cloud'],\n",
    "    'Spacecraft': ['telescope', 'spacecraft', 'probe', 'rover', 'satellite'],\n",
    "    'Astronomy Tools': ['astronomy', 'astrophysics', 'space exploration', 'observatory'],\n",
    "    'Eclipse': ['eclipse', 'solar eclipse', 'lunar eclipse'],\n",
    "    'Cosmology': ['cosmology', 'big bang', 'gravitational wave'],\n",
    "}\n",
    "\n",
    "# Classify celestial body types\n",
    "def classify_celestial_body(title, content):\n",
    "    title = title.lower()\n",
    "    content = content.lower()\n",
    "    \n",
    "    for celestial_type, keywords in celestial_types.items():\n",
    "        if any(keyword in title for keyword in keywords) or any(keyword in content for keyword in keywords):\n",
    "            return celestial_type\n",
    "    return 'Unknown'\n",
    "\n",
    "df_separat['Type'] = df_separat.apply(lambda row: classify_celestial_body(row['Title'], row['Cleaned_Content']), axis=1)\n",
    "\n",
    "# Remove rows with 'Unknown' Type if needed\n",
    "df_cleaned = df_separat[df_separat['Type'] != 'Unknown']\n",
    "\n",
    "# Save the cleaned dataset to a CSV file\n",
    "df_cleaned.to_csv('cleaned_celestial_bodies.csv', index=False)\n",
    "# Drob the columns Features and Content\n",
    "df_cleaned.drop(columns=['Features', 'Content'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d268a82-7e7c-401d-9dd9-e3dea5852f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a492fa-f728-4218-b4ad-0686874ae0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the Dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(df_cleaned.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc88db93-060b-4d5a-b9bc-a82519ea60c7",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">5.1.4 Summary Statistics and Type Distribution:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ce6af-304f-459a-8381-2507fa8c1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-Summary Statistics\n",
    "print(\"\\nSummary Statistics for Word Count:\")\n",
    "\n",
    "# Calculate word count for each cleaned content entry\n",
    "df_cleaned['Word_Count'] = df_cleaned['Cleaned_Content'].apply(lambda x: len(x.split()))  \n",
    "print(df_cleaned['Word_Count'].describe())\n",
    "\n",
    "# Type distribution\n",
    "print(\"\\nType Distribution:\")\n",
    "type_counts = df_cleaned['Type'].value_counts()\n",
    "print(type_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5832bfa1-7149-449d-89fe-2b4a43b79eb6",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">5.1.5 Encoding Celestial Body Types:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcf562e-283b-4224-a24b-fb59adb29d44",
   "metadata": {},
   "source": [
    "***************\n",
    "This step encodes the categorical Type column into numerical values to facilitate further analysis, such as machine learning or statistical modeling.\n",
    "****************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acac8ea-f268-4d01-8750-49a9998672d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df_cleaned['Type_Encoded'] = encoder.fit_transform(df_cleaned['Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5999fa2-a67a-43e9-94bf-7c9ba6394436",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef7be3-8941-4ea6-a981-1e78c9884c67",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">5.1.6 Data Cleaning and Structuring:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2774146a-966f-4512-89a9-26e070eef248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in the 'Content' column\n",
    "df_cleaned['Cleaned_Content'].fillna('', inplace=True)\n",
    "\n",
    "# Clean the 'Content' column\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Ensure the text is a string\n",
    "        return ''\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "    return text\n",
    "\n",
    "df_cleaned['Cleaned_Content'] = df_cleaned['Cleaned_Content'].apply(clean_text)\n",
    "\n",
    "# Define celestial body types and keywords\n",
    "celestial_types = {\n",
    "    'Planet': ['planet', 'jupiter', 'earth', 'mars', 'venus', 'saturn', 'uranus', 'neptune', 'dwarf planet'],\n",
    "    'Star': ['star', 'sun', 'nova', 'supernova', 'neutron star', 'red giant', 'pulsar', 'white dwarf'],\n",
    "    'Moon': ['moon', 'satellite', 'luna', 'natural satellite'],\n",
    "    'Asteroid': ['asteroid', 'comet', 'meteoroid', 'meteor', 'meteorite'],\n",
    "    'Galaxy': ['galaxy', 'milky way', 'andromeda', 'spiral galaxy', 'elliptical galaxy', 'irregular galaxy'],\n",
    "    'Nebula': ['nebula', 'emission nebula', 'reflection nebula', 'planetary nebula', 'dark nebula'],\n",
    "    'Black Hole': ['black hole', 'event horizon', 'singularity'],\n",
    "    'Constellation': ['constellation', 'zodiac'],\n",
    "    'Exoplanet': ['exoplanet', 'extrasolar planet'],\n",
    "    'Cosmic Structure': ['dark matter', 'dark energy', 'cosmos', 'universe', 'kuiper belt', 'oort cloud'],\n",
    "    'Spacecraft': ['telescope', 'spacecraft', 'probe', 'rover', 'satellite'],\n",
    "    'Astronomy Tools': ['astronomy', 'astrophysics', 'space exploration', 'observatory'],\n",
    "    'Eclipse': ['eclipse', 'solar eclipse', 'lunar eclipse'],\n",
    "    'Cosmology': ['cosmology', 'big bang', 'gravitational wave'],\n",
    "}\n",
    "\n",
    "# Classify celestial body types\n",
    "def classify_celestial_body(title, content):\n",
    "    title = title.lower()\n",
    "    content = content.lower()\n",
    "    \n",
    "    for celestial_type, keywords in celestial_types.items():\n",
    "        if any(keyword in title for keyword in keywords) or any(keyword in content for keyword in keywords):\n",
    "            return celestial_type\n",
    "    return 'Unknown'\n",
    "\n",
    "df_cleaned['Type'] = df_cleaned.apply(lambda row: classify_celestial_body(row['Title'], row['Cleaned_Content']), axis=1)\n",
    "\n",
    "# Remove rows with 'Unknown' Type if needed\n",
    "df_data = df_cleaned[df_cleaned['Type'] != 'Unknown']\n",
    "\n",
    "# Save the cleaned dataset to a CSV file\n",
    "df_data.to_csv('data_celestial_bodies.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a954593-3756-41e3-b5cf-2f3c7aa8287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d82f94a-9a77-497f-87ba-d57b4939f020",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">5.1.7 URL Validation:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d509b-2311-4564-9e73-e3dad95268a2",
   "metadata": {},
   "source": [
    "*********\n",
    "In this step, we validate the URLs in the dataset to ensure that they are properly formatted and accessible. This helps eliminate any malformed or incorrect URLs before further analysis.\n",
    "*********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a12100f-4787-468c-8a84-d1882dc22e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a URL is valid using urllib\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        result = urllib.parse.urlparse(url)\n",
    "        # Check if the URL has a valid scheme (http, https,...etc.) and netloc (domain)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "# Apply the URL validation function to the 'URL' column\n",
    "df_data['Is_Valid_URL'] = df_data['URL'].apply(is_valid_url)\n",
    "\n",
    "# Filter out invalid URLs\n",
    "df_data = df_data[df_data['Is_Valid_URL'] == True]\n",
    "df_data.to_csv('data_celestial_bodies.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f516cc89-c06a-4390-950d-37b574c0914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03edbf7-8b97-4d78-8703-bef135054f7a",
   "metadata": {},
   "source": [
    "## <span style=\"color: #A05899; font-size:1em;\">5.2 Image Data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac9105-58ff-44c6-ae3b-9e3c9c6b4e7b",
   "metadata": {},
   "source": [
    "*************\n",
    "This section will cover the extraction and saving of images from Wikipedia pages related to celestial bodies through scraping. The goal is to scrape the first valid image for each URL and save it in a folder for future use.\n",
    "*************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedfa15e-b6a4-41aa-97f1-ace48561c6f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Directory to save images\n",
    "image_dir = 'images'\n",
    "if not os.path.exists(image_dir):\n",
    "    os.makedirs(image_dir)\n",
    "\n",
    "# Function to extract the first valid image URL from a Wikipedia article\n",
    "def extract_first_main_image_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find all <img> tags with a valid src attribute\n",
    "            img_tags = soup.find_all('img', {'src': True})\n",
    "\n",
    "            valid_images = []\n",
    "            for img_tag in img_tags:\n",
    "                img_url = img_tag['src']\n",
    "                \n",
    "                # Exclude small images like favicons and only take jpg, jpeg, or png images\n",
    "                if 'favicon' not in img_url and (img_url.endswith('.jpg') or img_url.endswith('.jpeg') or img_url.endswith('.png')):\n",
    "                    img_url = urljoin(url, img_url)  # Ensure it's an absolute URL\n",
    "                    valid_images.append(img_url)\n",
    "\n",
    "            # Return the valid image URL \n",
    "            return valid_images[3] if valid_images else None\n",
    "        else:\n",
    "            print(f\"Error fetching page {url} - Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to download and save the first valid image as JPEG\n",
    "def download_image_as_jpeg(image_url, save_path):\n",
    "    try:\n",
    "        response = requests.get(image_url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            img.convert('RGB').save(save_path, 'JPEG')  # Save image as JPEG\n",
    "            return save_path\n",
    "        else:\n",
    "            print(f\"Failed to download image from {image_url}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading image from {image_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process all URLs in the DataFrame and return a new DataFrame with image paths\n",
    "def process_all_urls(df):\n",
    "    image_data = []  # A list to store image paths and corresponding titles\n",
    "    for index, row in df.iterrows():\n",
    "        # Extract the first valid image URL\n",
    "        image_url = extract_first_main_image_from_url(row['URL'])\n",
    "        \n",
    "        if image_url:\n",
    "            # Create a filename based on the title\n",
    "            img_filename = os.path.join(image_dir, f\"{row['Title'].replace(' ', '_')}.jpg\")\n",
    "            \n",
    "            # Download and save the image\n",
    "            downloaded_image = download_image_as_jpeg(image_url, img_filename)\n",
    "            \n",
    "            if downloaded_image:\n",
    "                image_data.append({'Title': row['Title'], 'Image_URL': image_url, 'Image_Saved': downloaded_image})  # Save the data in a dictionary\n",
    "            else:\n",
    "                image_data.append({'Title': row['Title'], 'Image_URL': image_url, 'Image_Saved': None})\n",
    "        else:\n",
    "            image_data.append({'Title': row['Title'], 'Image_URL': None, 'Image_Saved': None})\n",
    "    \n",
    "    # Create a new DataFrame to store the image paths and corresponding data\n",
    "    image_df = pd.DataFrame(image_data)\n",
    "\n",
    "    return image_df\n",
    "\n",
    "\n",
    "# Process all URLs and download images into a new DataFrame\n",
    "df_image = process_all_urls(df_data)\n",
    "\n",
    "# Save the new DataFrame with image data to a CSV file\n",
    "df_image.to_csv('Image_celestial_bodies.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96891a22-bd67-4896-bcb2-2e627ea3b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd0428-bfdf-464c-8c55-bbd96a947608",
   "metadata": {},
   "source": [
    "<div style=\" border: 5px solid #AC4d61; border-radius: 20px;\">\n",
    "    <h2 style=\"margin: auto; padding: 20px;color: #AC4d61;font-size:1.8em;\">6. Data Loading and Cleaning</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded8de4-6a63-4b51-9365-d1a82621711a",
   "metadata": {},
   "source": [
    "*************\n",
    "**Objective:**\n",
    "In this step, we will merge the dataset we collected and cleaned .\n",
    "*************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be41e76-ac86-42a3-831e-339f7f9c03d8",
   "metadata": {},
   "source": [
    "## <span style=\"color: #A05899; font-size:1em;\">6.1 Load The Data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dff2eb-0a25-42bc-873a-ec5344e5cf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your cleaned dataset\n",
    "df1 = pd.read_csv('data_celestial_bodies.csv')\n",
    "# Load the outher dataset \n",
    "df2 = pd.read_csv('data_celestial_bodies2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29704cc-f502-4bf8-9141-a3f414ddf00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset 1 (df1):\")\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab89b0b-1163-46dd-a843-55e63c0e7d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset 2 (df2):\")\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eea910-79f8-4071-a7e1-f56c4f94abbf",
   "metadata": {},
   "source": [
    "## <span style=\"color: #A05899; font-size:1em;\">6.2 Merge DataFrames</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf9a05e-cb7c-4d5f-a8a6-acecf6f68b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two datasets\n",
    "df_merged = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55687673-480a-416f-aaf4-f90356e2002e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the merged dataset to check the results\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f5b9a-b0c3-4e89-9dae-5b7406f52ea5",
   "metadata": {},
   "source": [
    "## <span style=\"color: #A05899; font-size:1em;\">6.3  Data Cleaning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343ca966-1992-48d9-92bc-3acd01e904f7",
   "metadata": {},
   "source": [
    "#### ***Check the the merged dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6c505-d8ec-4f9b-bfc7-af1141a6fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Overview:\")\n",
    "print(df_merged.info()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0825372-6cb5-4203-ae53-4a2305fc2cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values by Column:\")\n",
    "print(df_merged.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc19e8f-d1f8-446a-a8da-1d9c99f671cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\Column Names in Dataset:\")\n",
    "print(df_merged.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725669a7-b241-4dcc-9824-8987178bd7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_merged[df_merged['Is_Valid_URL'] == False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa95f04-4f49-4455-84bb-4ae2a3c806c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate = df_merged[df_merged['Title'].duplicated(keep=False)]\n",
    "print(f\"Number of duplicate entries: {duplicate.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3fea81-b6de-4d17-80ed-ee84bd4476d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group duplicate entries by title and display them\n",
    "group_duplicate = duplicate.groupby('Title')\n",
    "\n",
    "for title, group in group_duplicate:\n",
    "    print(f\"\\nTitle: {title}\")\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae1930b-c265-4e22-ad80-42f2f5b2e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.drop_duplicates(subset='Title', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f54e82c-14aa-4a5c-b552-3ca433f7d5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicate in Title Column:\")\n",
    "print(df_merged['Title'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e59b021-71e2-4350-a1b4-5d9efab94082",
   "metadata": {},
   "source": [
    "## <span style=\"color: #A05899; font-size:1em;\">6.4  Save the Merged Data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852cf533-0707-47e6-9f0d-26ee90224d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged DataFrame to a new CSV file\n",
    "df_merged.to_csv('merged_celestial_bodies.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c9d405-01df-4535-a3e8-9e3433603a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c27c4-8b9e-40ee-94c7-6d018bec573b",
   "metadata": {},
   "source": [
    "<div style=\" border: 5px solid #AC4d61; border-radius: 20px;\">\n",
    "    <h2 style=\"margin: auto; padding: 20px;color: #AC4d61;font-size:1.8em;\">7. Exploratory Data Analysis (EDA)</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2468f943-ca30-4daf-9e32-cb51832500e9",
   "metadata": {},
   "source": [
    "***\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3198dfdd-c37b-40a8-ae30-303250591e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_csv('merged_celestial_bodies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e8e6f4-1286-4df6-8b5d-ec9205028ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the dataset\n",
    "print(f\"Dataset contains {df_merged.shape[0]} rows and {df_merged.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97297cf3-a549-4586-a57e-e0e41b16f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(df_merged.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75a86d7-9def-4486-ad41-053a3e4b3cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column data types\n",
    "print(df_merged.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5a6cab-548f-457f-a783-d347c53e56df",
   "metadata": {},
   "source": [
    "*******\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fc9702-3e5c-43b9-9807-10f8788ae44d",
   "metadata": {},
   "source": [
    "## <span style=\"color: #A05899; font-size:1em;\">7.1 Overview of Textual Data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a4d4d3-6993-46c7-b87d-98399db4c198",
   "metadata": {},
   "source": [
    "**************\n",
    "In this step, we will explore the textual data in the dataset, focusing on the Cleaned_Content column. The goal is to find useful insights and patterns that may lead to further analysis or modeling. Textual data analysis involves assessing word frequency, finding trends, distributions, and relationships.\n",
    "**************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cdd2f8-2af0-4e6c-b377-556794d7f539",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">7.1.1 Word Count Distribution:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8547f26c-b493-4619-8c07-376d0b2e8463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the summary statistics of word counts\n",
    "print(\"\\nSummary Statistics for Word Count:\")\n",
    "print(df_merged['Word_Count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61366a40-ad8a-4097-9a8f-db4e626a59b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of word counts using a histogram\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df_merged['Word_Count'], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Word Counts in Cleaned Content')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d9e4d-ea55-4297-8665-bf99b5688d27",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">7.1.2 Most Common Keywords by Celestial Body Type:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b83b9f-bb70-4837-93e0-84012965c4db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a count vectorizer to get word frequencies for each celestial body type\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=10)\n",
    "\n",
    "# Group by 'Type' and apply vectorizer to get word frequencies for each type\n",
    "type_keywords = df_merged.groupby('Type')['Cleaned_Content'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Generate word frequency count for each type\n",
    "for idx, row in type_keywords.iterrows():\n",
    "    word_counts = vectorizer.fit_transform([row['Cleaned_Content']])\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    counts = word_counts.toarray().sum(axis=0)\n",
    "    \n",
    "    # Create a DataFrame with word frequencies\n",
    "    word_freq = pd.DataFrame(list(zip(words, counts)), columns=['Word', 'Frequency'])\n",
    "    \n",
    "    # Plot bar chart for the most common words in each type\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Frequency', y='Word', data=word_freq.sort_values(by='Frequency', ascending=False))\n",
    "    plt.title(f'Most Common Keywords in {row[\"Type\"]}')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Word')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa2f4f-fe37-47eb-a484-839c9c1e4eb2",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">7.1.3 Check for Irrelevant Data:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0360508-2c24-4254-872d-a08fd37ff7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 25th percentile \n",
    "word_threshold = df_merged['Word_Count'].quantile(0.25)  \n",
    "\n",
    "# Now filter rows with very short content\n",
    "short_content = df_merged[df_merged['Word_Count'] < word_threshold]\n",
    "print(f\"Rows with very short content: {short_content.shape[0]}\")\n",
    "print(\"\\n\", short_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc9a67-4fbe-4007-b616-681ac00dc200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for generic or irrelevant Titles\n",
    "irrelevant_titles = ['Home', 'Wikipedia', 'Main Page']  \n",
    "irrelevant_title_rows = df_merged[df_merged['Title'].isin(irrelevant_titles)]\n",
    "print(f\"Rows with generic or irrelevant titles: {irrelevant_title_rows.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d611ba3-fedb-49ba-b01d-a13841bd12b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for rows with unknown or unexpected types\n",
    "unexpected_types = ['Unknown', 'Other']  # Add types that don't fit your analysis\n",
    "unexpected_type_rows = df_merged[df_merged['Type'].isin(unexpected_types)]\n",
    "print(f\"Rows with unexpected types: {unexpected_type_rows.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05929468-cfc0-4276-902d-4386673a62cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with word count less than the threshold\n",
    "df_merged = df_merged[df_merged['Word_Count'] >= word_threshold]\n",
    "\n",
    "#shape of the cleaned DataFrame\n",
    "print(f\"Shape of cleaned dataset: {df_merged.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c055bf-f3a5-4039-a50a-3b8d80b39c2f",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">7.1.4 Check for The 'Type' and 'Type_Encoded':</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777b5cbc-b2ef-4a04-95de-643f1edaf6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values in 'Type'\n",
    "print(\"Unique values in 'Type' column:\", df_merged['Type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e56abdc-98b9-4dc7-ac5d-6f1ef3da487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for celestial body types\n",
    "type_counts = df_merged['Type'].value_counts()\n",
    "\n",
    "# Define the labels and values\n",
    "labels = type_counts.index\n",
    "sizes = type_counts.values\n",
    "colors = sns.color_palette('Set3', len(labels)).as_hex() \n",
    "\n",
    "# Plotting the pie chart with enhancements\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Create the pie chart\n",
    "patches, texts, pcts = ax.pie(sizes, labels=labels, autopct='%.1f%%', colors=colors, shadow=True, startangle=90, wedgeprops={'linewidth': 3.0, 'edgecolor': 'white'})\n",
    "\n",
    "# Add a legend\n",
    "ax.legend(patches, labels, loc=\"best\", fontsize=7, frameon=False)\n",
    "\n",
    "# Make sure the pie chart is a perfect circle\n",
    "ax.axis('equal')\n",
    "\n",
    "# Set the title and adjust the layout\n",
    "ax.set_title('Distribution of Celestial Body Types', fontsize=18, weight='bold')\n",
    "\n",
    "# Adjust the text properties\n",
    "plt.setp(texts, fontweight='bold', fontsize=13, color='white')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc98d51-dd8b-4c7b-affb-641d691b290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of 'Type_Encoded'\n",
    "encoded_counts = df_merged['Type_Encoded'].value_counts()\n",
    "print(\"Counts of each 'Type_Encoded':\")\n",
    "print(encoded_counts)\n",
    "print(\"******************************************************************\\n\")\n",
    "# Get unique mappings between 'Type' and 'Type_Encoded'\n",
    "type_mapping = df_merged[['Type', 'Type_Encoded']].drop_duplicates()\n",
    "print(\"Mapping between 'Type' and 'Type_Encoded':\")\n",
    "print(type_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5697559-c3e5-43cf-9977-ea90fb8f4339",
   "metadata": {},
   "source": [
    "***************\n",
    "The Type_Encoded values show that planet (6) is the most common encoded value, while spacecraft (7) and black hole (1) have relatively fewer entries.\n",
    "**************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63dc776-5293-45f9-bfe2-8c5834fae067",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=type_counts.index, y=type_counts.values, palette=\"viridis\")\n",
    "plt.title(\"Counts of Each Type\", fontsize=16, fontweight=\"bold\")\n",
    "plt.xlabel(\"Type\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7818a92e-1edc-4b3f-84f2-cb4c9d1a5ba0",
   "metadata": {},
   "source": [
    "## <span style=\"color: #A05899; font-size:1em;\">7.2 Overview of Image Data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563785d0-c9ce-4a65-a8f6-680e279e28c7",
   "metadata": {},
   "source": [
    "*************\n",
    "In this section, we will explore the image data related to celestial bodies. The idea is to review the spread of images in our dataset, check whether the images have been fetched appropriately, and verify whether any data of images is missing or inconsistent.\n",
    "\n",
    "The following items will be presented:\n",
    "1. **Image Availability**\n",
    "2. **Visualization of Image Distribution**\n",
    "3. **Image Quality and Format**\n",
    "************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa2cfae-84f5-4fc8-af14-3b0ae0c5436e",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">7.2.1 Image Availability:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3622744c-f18b-4bbd-8b2c-58c0933ac794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your cleaned dataset\n",
    "df_image = pd.read_csv('Image_celestial_bodies.csv')\n",
    "\n",
    "# Load the outher dataset \n",
    "df_image2= pd.read_csv('wikipedia_photos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2d4bb1-76a4-4192-8330-3193dceabce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7906077-f6f3-484d-bec6-5802648b8ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca49289b-148b-467e-ae86-0c0386b61641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing image paths\n",
    "missing_images = df_image[df_image['Image_Saved'].isnull()]\n",
    "missing_images2 = df_image2[df_image2['photo_description'].isnull()]\n",
    "\n",
    "# Display the count of missing images\n",
    "print(f\"Number of missing images: {missing_images.shape[0]}\")\n",
    "print(f\"Number of missing images2: {missing_images2.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d3afcc-89d0-439b-b426-c56b00d5f1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the duplicated\n",
    "df_image2=df_image2[~df_image2['photo_url'].duplicated()]\n",
    "df_image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff8413-12e7-47c7-ab8f-a1f219ef53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in df_image2\n",
    "df_image2 = df_image2.rename(columns={'title':'Title','photo_url': 'Image_URL', 'photo_description': 'Image_Saved'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5490ed-a118-466c-b1b9-2ca70d2ec681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two datasets\n",
    "df_merged_image = pd.concat([df_image, df_image2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89df28-4408-4f20-9875-42ec174bdbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the merged dataframe\n",
    "print(df_merged_image.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1655a8-f663-47a9-b196-6853c5a58f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing image URLs or image descriptions in df_image2\n",
    "df_merged_image = df_merged_image.dropna(subset=['Image_URL', 'Image_Saved'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402bb33-0846-4dd8-b241-a46fbfd98f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c778fd14-1609-46b0-be9e-1679f6f0cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged dataframe to a new CSV file \n",
    "df_merged_image.to_csv('merged_images_celestial_bodies.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ae340-ca90-481d-b601-6422df53b638",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">7.2.2 Visualization of Image Distribution:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c34d7f-9c16-4f68-9122-45bcf37bbf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check image availability\n",
    "valid_images = df_merged_image[df_merged_image['Image_Saved'].notna()]\n",
    "print(valid_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a656ee7-baad-482f-89d7-8992fea024df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the image based on user input title\n",
    "def display_image_by_title(title):\n",
    "    # Search for the title in the DataFrame\n",
    "    result = valid_images[valid_images['Title'].str.contains(title, case=False, na=False)]\n",
    "    \n",
    "    if not result.empty:\n",
    "        img_path = result['Image_Saved'].values[2]  \n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        # Display the image\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Image of {title}\")\n",
    "        plt.axis('off')  \n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No image found for the title: {title}\")\n",
    "\n",
    "# Get the title from the user\n",
    "user_input = input(\"Enter the title of the celestial body: \")\n",
    "display_image_by_title(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041c0acc-ca1e-49b6-9ba9-5d5ce7668c56",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">7.2.3 Image Quality and Format:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1fbc15-ce00-4eb6-983b-5032cb33d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check image format and size\n",
    "def check_image_format_and_size(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        img_format = img.format\n",
    "        img_size = img.size \n",
    "        return img_format, img_size\n",
    "    except Exception as e:\n",
    "        return None, None\n",
    "\n",
    "# Apply the function to check image format and size\n",
    "df_merged_image['Image_Format'], df_merged_image['Image_Size'] = zip(*df_merged_image['Image_Saved'].apply(check_image_format_and_size))\n",
    "\n",
    "# Display images with invalid format or size (if any)\n",
    "invalid_images = df_merged_image[df_merged_image['Image_Format'].isnull() | df_merged_image['Image_Size'].isnull()]\n",
    "print(f\"Invalid Images:\\n{invalid_images[['Title', 'Image_Saved']]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2832919e-0469-4d4b-83d5-eb64aa2083b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_image.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2446e6c9-72ab-4024-be79-7762166c072a",
   "metadata": {},
   "source": [
    "## <span style=\"color: #A05899; font-size:1em;\">7.3 Identifying Trends and Patterns</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e959e-315f-415f-9ec5-2ceac4acb529",
   "metadata": {},
   "source": [
    "*********\n",
    "Next comes the identification of key trends and patterns in the data. It is here that we hope to find out any great relationships or, at best, insights from both the textual and numerical data for the stars. Once identified, these will help in making data-driven observations with regards to what the dataset actually contains and how it is structured.\n",
    "********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7231cd8e-8e18-4588-9065-585614121332",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">7.3.1 Word Cloud Visualization:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c29c58b-895d-43d8-80c6-e4e86ba60406",
   "metadata": {},
   "source": [
    "*Textual Analysis of Celestial Body Descriptions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b33d8c-75ae-4153-a9e9-e43c418bd7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Word Cloud for each celestial body type\n",
    "def generate_wordcloud_by_type(df):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    types = df['Type'].unique()\n",
    "\n",
    "    for i, celestial_type in enumerate(types):\n",
    "        content = df[df['Type'] == celestial_type]['Cleaned_Content'].str.cat(sep=' ')\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(content)\n",
    "\n",
    "        plt.subplot(3, 4, i + 1)\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(f'Word Cloud for {celestial_type}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "generate_wordcloud_by_type(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c48ed-bc8e-43a8-8c45-4d5e70c20f87",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">7.3.2 Trends in Word Count Across Different TypesÂ¶:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b409da4a-c722-4f33-86fb-d0268b4e6932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot to show distribution of word count across celestial types\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Type', y='Word_Count', data=df_merged, palette='Set2')\n",
    "plt.title('Word Count Distribution Across Celestial Body Types')\n",
    "plt.xlabel('Celestial Body Type')\n",
    "plt.ylabel('Word Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c501913-2bac-4efa-98a2-f543d45c85ab",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">7.3.3 Exploring Patterns Between Types and Features:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7d371-bbc8-4243-9013-284b3741837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring patterns between 'Type' and 'Word_Count'\n",
    "type_word_count = df_merged.groupby('Type')['Word_Count'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Plot the average word count for each type\n",
    "plt.figure(figsize=(10, 6))\n",
    "type_word_count.plot(kind='bar', color='skyblue')\n",
    "plt.title('Average Word Count for Each Celestial Body Type')\n",
    "plt.xlabel('Celestial Body Type')\n",
    "plt.ylabel('Average Word Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91acb535-d089-4ccb-a763-a03170c479c6",
   "metadata": {},
   "source": [
    "## <span style=\"color: #A05899; font-size:1em;\">7.4 Correlation between features</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bcc65b-0201-4c16-982d-432a9292c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column data types\n",
    "print(df_merged.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1f39d-b1ee-4944-863f-b6b5bcdcadb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the correlation matrix for numeric columns\n",
    "correlation_matrix = df_merged.corr(numeric_only=True)\n",
    "# Display the correlation matrix\n",
    "print(\"Correlation Matrix:\\n\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm',  linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix of Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3412c66f-0c66-4878-ab42-78d3bb19019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Word_Count\", \"Type_Encoded\"]\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df_merged[feature], kde=True, bins=30, color=\"teal\")\n",
    "    plt.title(f\"Distribution of {feature}\", fontsize=16, fontweight=\"bold\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0acc8fc-f89e-426c-84bb-c5ce48f871a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    df_merged[\"Word_Count\"],\n",
    "    df_merged[\"Type_Encoded\"],\n",
    "    s=df_merged[\"Word_Count\"] / 10,\n",
    "    alpha=0.6,\n",
    "    c=df_merged[\"Type_Encoded\"],\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "plt.colorbar(label=\"Type Encoded\")\n",
    "plt.title(\"Bubble Chart: Word Count vs. Type Encoded\", fontsize=16, fontweight=\"bold\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Type Encoded\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27cd9cc-309a-4fc7-a20f-856485ce055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation between Word Count and Type Encoded\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='Word_Count', y='Type_Encoded', data=df_merged, hue='Type', palette='Set1')\n",
    "plt.title('Correlation Between Word Count and Celestial Body Type Encoded')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Type Encoded')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21952809-ff16-48b8-b668-0bfe7e5dbe32",
   "metadata": {},
   "source": [
    "******************\n",
    "- Given the weak correlation between Word_Count and Type_Encoded, it is quite obvious that word count is not a good predictor of the type of celestial body. It could mean that the content length does not differ much based on whether the entry refers to a planet, star, or other types of celestial body.\n",
    "- This is expected and reflects the fact that the validity of a URL is independent of word count or type of celestial body.\n",
    "****************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b052efe-480a-4455-97c3-7f471a772324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged DataFrame to a new CSV file\n",
    "df_merged.to_csv('merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c70f66a-7243-41a5-ac97-e3adf159edf5",
   "metadata": {},
   "source": [
    "<div style=\" border: 5px solid #AC4d61; border-radius: 20px;\">\n",
    "    <h2 style=\"margin: auto; padding: 20px;color: #AC4d61;font-size:1.8em;\">8. Model Building and Deployment</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2109d25-eb52-4b7d-b7b0-1fc8928cea4c",
   "metadata": {},
   "source": [
    "***********\n",
    "In this step, you will perform the following tasks:\n",
    "\n",
    "- Data Preparation.\n",
    "- Model Selection.\n",
    "- Model Training and Evaluation.\n",
    "- Saving the Model.\n",
    "- Deployment.\n",
    "***********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377675b5-557c-4dce-9f1a-47488bfeb3fa",
   "metadata": {},
   "source": [
    "## <span style=\"color: #A05899; font-size:1em;\">8.1 Data Preparation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6ad1ac-2f79-4ec9-b35f-6a7ae7bb16a6",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">8.1.1 Feature Selection:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f2e37a-c93b-401d-9e41-1e99905dc357",
   "metadata": {},
   "source": [
    "*******\n",
    "*For the model, I chose the following features:*\n",
    "\n",
    "- 'Word_Count':\n",
    "Reason: The number of words in the description may indicate the level of detail about the celestial body and, therefore, might be related to its type.\n",
    "- 'Type_Encoded':\n",
    "Reason: This numerical encoding of the 'Type' feature allows the model to use the target variable, or celestial body types, as a feature to predict on.\n",
    "- **Target Variable**\n",
    "- 'Type':\n",
    "Reason: This is the category we are trying to predict, such as 'planet', 'star'.\n",
    "- **Why These Features?**\n",
    "- 'Word_Count' and 'Type_Encoded' are selected because they are numerical and directly related to the prediction of 'Type' of celestial bodies.\n",
    "*********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020994be-26db-4b0f-881d-53c04ed8d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('merged_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666cdcb-3a18-46c0-9def-ff9b59d49271",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">8.1.2 Prepare features:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b774f-cc2c-4857-b8ca-a50946d51c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in 'Type' and 'Type_Encoded'\n",
    "missing_values = df_data[['Word_Count', 'Type_Encoded']].isnull().sum()\n",
    "print(\"Missing values in 'Type' and 'Type_Encoded':\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3db812-2095-49ed-908a-bf023131ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature encoding\n",
    "X = df_data[['Word_Count', 'Type_Encoded']] \n",
    "y = df_data['Type']  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee064ab-7e7a-46e3-8b32-8e43e37af135",
   "metadata": {},
   "source": [
    "<h3 style=\"margin: auto; padding: 20px; color: RGB(100,40,80); \">8.1.3 Feature scaling:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79005050-7272-4713-bd38-5d41adcb8631",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# use scaling to the features\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# checking the mean and standard deviation after scaling\n",
    "print(\"Mean after scaling:\", X_scaled.mean(axis=0))  # close to 0\n",
    "print(\"Standard deviation after scaling:\", X_scaled.std(axis=0)) # close to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcbbe5e-cb75-45b8-b67b-458e746b9652",
   "metadata": {},
   "source": [
    "*******************************************\n",
    "**Scaling Results Explanation**\n",
    "After scaling the features using StandardScaler, here is what we observe:\n",
    "\n",
    "- **Mean after scaling:**\n",
    "\n",
    "[ 8.33e-17, -2.99e-16]\n",
    "Explain: the mean of both features are now very close to 0, which is what we expect when using StandardScaler. For floating-point precision, the number can be close to zero but considered effectively zero.\n",
    "\n",
    "- **Standard deviation after scaling:**\n",
    "\n",
    "[1.0, 1.0]\n",
    "Reason: The standard deviation for both features is 1, which confirms that the scaling has been applied correctly. That means that now both features are on the same scale and can be compared to each other, for example, by any machine learning model.\n",
    "Why Scaling is Important\n",
    "\n",
    "--------------------------------------------------------------------------------------------------\n",
    "\n",
    "is Important\n",
    "Scaling is important because it puts all the features into the same range, making the model less sensitive to the scale of the data. It also ensures that no feature would dominate the model due to having a larger scale-for example, Word_Count in thousands compared with Type_Encoded in a small range. Scaling allows the model to work faster and often converge faster du\n",
    "*************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3db0ca-0d6e-47d3-a23d-413969ba8b40",
   "metadata": {},
   "source": [
    "## <span style=\"color: #A05899; font-size:1em;\">8.2 Model Selection</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f1567-44db-477f-998b-a01a00ed1cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the models to be compared\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "model_results = []\n",
    "\n",
    "# Evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "    \n",
    "    # Store the results\n",
    "    model_results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1\n",
    "    })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "model_results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# Display the results as a table\n",
    "print(model_results_df)\n",
    "\n",
    "# Select the best model based on accuracy\n",
    "best_model_name = model_results_df.loc[model_results_df['Accuracy'].idxmax(), 'Model']\n",
    "best_model = models[best_model_name]\n",
    "print(f\"\\nBest Model: {best_model_name} (Accuracy: {model_results_df['Accuracy'].max():.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e02a5-4703-4382-bcea-c46536c9d515",
   "metadata": {},
   "source": [
    "**********************\n",
    "**Best Model Selection**\n",
    "The comparison above clearly illustrates that the Random Forest model outperforms others on the highest values of accuracy (0.9900), precision (0.9950), recall (0.9900), and F1-score (0.9876); therefore, this model performed the best among all these features for this particular task.\n",
    "***********************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8798158-8aaf-4303-84c5-9f5d2b727288",
   "metadata": {},
   "source": [
    "#### ***the classification report***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7406fa12-1b43-4ee2-a4b0-1aa553408239",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"\\n{model_name} Classification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n*******************************************************************************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a81a47-aaec-4648-9fdf-6382f6f5ab96",
   "metadata": {},
   "source": [
    "__________________\n",
    "- Therefore, Random Forest is the overall better-performing model due to its precision, recall, F1-score, and the performance balance among all categories. It's particularly good for categories such as \"Planet\", \"Star\", and \"Spacecraft\".\n",
    "\n",
    "- Logistic Regression has an overall lower accuracy, with very high errors in identifying \"Moon\" and \"Spacecraft\". This is evident from its low precision and recall values for those categories, hence not being that reliable on these types.\n",
    "_______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b885bd-d3ab-4196-8f12-337c32162530",
   "metadata": {},
   "source": [
    "#### ***Model Accuracy vs. Other Metrics***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be658098-e87f-4d06-af92-ee3364a9a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and plot model metrics\n",
    "metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"Precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "    \"Recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "    \"F1-Score\": f1_score(y_test, y_pred, average='weighted')\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(list(metrics.items()), columns=[\"Metric\", \"Value\"])\n",
    "\n",
    "# Plot the evaluation metrics\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='Metric', y='Value', data=metrics_df, palette='viridis')\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b802d6-f60f-4b0a-b80b-c80eedb6087e",
   "metadata": {},
   "source": [
    "## <span style=\"color: #A05899; font-size:1em;\">8.3 Saving the Model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb26cc-e2fa-494c-99e3-efbf272bc7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "joblib.dump(best_model, 'best_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bec4dab-291b-4037-957c-e6a3244ca0cb",
   "metadata": {},
   "source": [
    "## <span style=\"color: #A05899; font-size:1em;\">8.4 Deployment</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1522eb-2d65-428d-a8c6-9f70c7417a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and scaler\n",
    "model = joblib.load('best_model.pkl')\n",
    "scaler = joblib.load('scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168d2a1-e3d9-4101-b355-6bb3e35aba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the type and URL for all matching celestial bodies\n",
    "def predict_celestial_body(name):\n",
    "    # Search for all rows where the 'title' contains the provided name\n",
    "    matches = df_data[df_data['Title'].str.contains(name, case=False, na=False)]\n",
    "    \n",
    "    if matches.empty:\n",
    "        return f\"No data found for '{name}'.\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Loop through all matches and collect relevant data (Type and URL)\n",
    "    for index, match in matches.iterrows():\n",
    "        # Process the data for prediction \n",
    "        word_count = match['Word_Count']\n",
    "        type_encoded = match['Type_Encoded']\n",
    "        features = scaler.transform([[word_count, type_encoded]])\n",
    "        \n",
    "        # Predict the type using the model\n",
    "        predicted_type = model.predict(features)[0]\n",
    "        predicted_url = match['URL']\n",
    "        \n",
    "        results.append(f\"Predicted Type: {predicted_type}\\nURL: {predicted_url}\\n\")\n",
    "    \n",
    "    # Return all the predictions for the matching rows\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "name = input(\"Enter a celestial body name: \")\n",
    "print(predict_celestial_body(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2752c8d4-63f7-4f06-b0f4-38897c84e937",
   "metadata": {},
   "source": [
    "<div style=\" border: 5px solid #AC4d61; border-radius: 20px;\">\n",
    "    <h2 style=\"margin: auto; padding: 20px;color: #AC4d61;font-size:1.8em;\">9. Conclusion</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05ad4aa-d5e1-4b35-9dd3-182d2181d1ac",
   "metadata": {},
   "source": [
    "_____________________\n",
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b03be2-ab3f-4098-8bbf-fa24a5e49c24",
   "metadata": {},
   "source": [
    "In This notebook , we from collecting data on a celestial body to cleaning that data and then EDA and building a model. Following these steps, we have seen how insight can be properly extracted from the dataset by culminating in a sturdy machine learning model.\n",
    "\n",
    "#### Key Achievements:\n",
    "\n",
    "1. **Data Collection and Structuring:**\n",
    "- We successfully scraped and structured a dataset containing both textual and image data related to celestial bodies. This allowed us to associate textual descriptions with corresponding visual characteristics, providing a rich foundation for further analysis.\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA):**\n",
    "- In the EDA, **stars** and **planets** were the most common body types within the dataset, whereas **galaxies** are related to more specific details and high-resolution images.\n",
    "The positive correlation of the length of textual descriptions with the resolution of images reflects the fact that more complex celestial bodies, such as galaxies.\n",
    "\n",
    "3. **Modeling Insights:\n",
    "The best performance was from the model with the **Random Forest**, having an accuracy of **99.01%**; thus, it can classify most texts  correctly into their respective classes. Indeed, for most classes, precision and recall are very good. This model proves to be really solid and reliable to classify most data in this or any similar tasks on star classification in the near future.\n",
    "\n",
    "#### Final Thoughts:\n",
    "This analysis points to the necessity of considering both text and image data for better comprehension of complex entities like those from outer space. The good performance of the Random Forest model hints at its use in a wide range of future applications, such as auto-classification of celestial objects through their description and related images. Going forward, there is always room for improving the performance by either including additional features or more advanced techniques, like deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0b673f-5f03-4a08-81c3-3181e736f342",
   "metadata": {},
   "source": [
    "_______________________________________________\n",
    "_________________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
